{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aacce83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855faf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"fraudTrain.csv\")\n",
    "test_df = pd.read_csv(\"fraudTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a3ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed: 0', 'cc_num', 'first', 'last', 'gender', 'street', 'city', 'state',\n",
    "             'zip', 'job', 'dob', 'trans_num', 'merchant']\n",
    "\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1697c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Columns :  ['category', 'amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud', 'trans_date_ts', 'category_index']\n",
      "Types of category :  ['entertainment', 'food_dining', 'gas_transport', 'grocery_net', 'grocery_pos', 'health_fitness', 'home', 'kids_pets', 'misc_net', 'misc_pos', 'personal_care', 'shopping_net', 'shopping_pos', 'travel']\n",
      "Count of category :  category\n",
      "gas_transport     131659\n",
      "grocery_pos       123638\n",
      "home              123115\n",
      "shopping_pos      116672\n",
      "kids_pets         113035\n",
      "shopping_net       97543\n",
      "entertainment      94014\n",
      "food_dining        91461\n",
      "personal_care      90758\n",
      "health_fitness     85879\n",
      "misc_pos           79655\n",
      "misc_net           63287\n",
      "grocery_net        45452\n",
      "travel             40507\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train_df.head(5)\n",
    "print(\"All Columns : \",train_df.columns.tolist())\n",
    "print(\"Types of category : \",sorted(train_df['category'].unique()))\n",
    "print(\"Count of category : \",train_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0432ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime to UNIX timestamp (in seconds)\n",
    "train_df['trans_date_ts'] = pd.to_datetime(train_df['trans_date_trans_time']).view('int64') // 10**9\n",
    "test_df['trans_date_ts'] = pd.to_datetime(test_df['trans_date_trans_time']).view('int64') // 10**9\n",
    "\n",
    "# Drop original datetime columns\n",
    "train_df.drop(columns=['trans_date_trans_time'], inplace=True)\n",
    "test_df.drop(columns=['trans_date_trans_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e017f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category'] = train_df['category'].fillna('unknown')\n",
    "test_df['category'] = test_df['category'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2faadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical feature\n",
    "le = LabelEncoder()\n",
    "train_df['category_index'] = le.fit_transform(train_df['category'])\n",
    "test_df['category_index'] = le.transform(test_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc22130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "features = ['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat',\n",
    "            'merch_long', 'trans_date_ts', 'category_index']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['is_fraud']\n",
    "X_test = test_df[features]\n",
    "y_test = test_df['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0b9efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# log_reg = LogisticRegression(\n",
    "#     max_iter=200,            # The maximum number of iterations taken for the solvers to converge\n",
    "#     solver='lbfgs',          # Optimization algorithm. 'lbfgs' is efficient for large datasets\n",
    "#     penalty='l2',            # Regularization technique ('l1', 'l2', 'elasticnet', 'none')\n",
    "#     C=1.0,                   # Inverse of regularization strength; smaller values specify stronger regularization\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# random_forest = RandomForestClassifier(\n",
    "#     n_estimators=20,         # The number of trees in the forest\n",
    "#     max_depth=10,            # Maximum depth of each tree (limits overfitting)\n",
    "#     max_features='sqrt',     # Number of features to consider for splitting at each node (sqrt or 'auto' for classification)\n",
    "#     min_samples_split=2,     # The minimum number of samples required to split an internal node\n",
    "#     min_samples_leaf=1,      # The minimum number of samples required to be at a leaf node\n",
    "#     n_jobs=-1,               # Number of CPU cores to use (-1 means using all cores)\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# gradient_boost = GradientBoostingClassifier(\n",
    "#     n_estimators=20,        # Number of boosting stages\n",
    "#     learning_rate=0.1,      # Step size shrinking to prevent overfitting\n",
    "#     max_depth=3,            # Maximum depth of individual trees (tree complexity)\n",
    "#     min_samples_split=2,    # Minimum samples required to split a node\n",
    "#     min_samples_leaf=1,     # Minimum samples required at leaf node\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# decision_tree = DecisionTreeClassifier(\n",
    "#     criterion='gini',       # The function to measure the quality of a split ('gini' or 'entropy')\n",
    "#     max_depth=10,           # Maximum depth of the tree\n",
    "#     min_samples_split=2,    # Minimum number of samples required to split a node\n",
    "#     min_samples_leaf=1,     # Minimum number of samples required at a leaf node\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# mlp = MLPClassifier(\n",
    "#     hidden_layer_sizes=(50, 25),   # The number of neurons in each hidden layer\n",
    "#     activation='relu',              # Activation function for the hidden layers ('relu', 'tanh', 'logistic')\n",
    "#     solver='adam',                  # The solver for weight optimization ('adam', 'sgd', 'lbfgs')\n",
    "#     max_iter=200,                   # Maximum number of iterations\n",
    "#     alpha=0.0001,                   # L2 penalty (regularization term)\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# svc = SVC(\n",
    "#     kernel='linear',             # Specifies the kernel type ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "#     C=1.0,                       # Regularization parameter. Higher values = less regularization\n",
    "#     gamma='scale',               # Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels\n",
    "#     probability=True,            # Enable probability estimates for prediction\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# xgboost_model = xgb.XGBClassifier(\n",
    "#     n_estimators=50,          # Number of boosting rounds\n",
    "#     max_depth=6,              # Maximum depth of each tree\n",
    "#     learning_rate=0.1,        # Step size for each boosting round\n",
    "#     subsample=0.8,            # Fraction of training samples to use for each tree\n",
    "#     colsample_bytree=0.8,     # Fraction of features to use for each tree\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# models = {\n",
    "#     \"Logistic Regression\": log_reg,\n",
    "#     \"Random Forest\": random_forest,\n",
    "#     \"Gradient Boosted Trees\": gradient_boost,\n",
    "#     \"Decision Tree\": decision_tree,\n",
    "#     \"MLP Classifier\": mlp,\n",
    "#     \"Linear SVM\": svc,\n",
    "#     \"XGBoost\": xgboost_model\n",
    "# }\n",
    "\n",
    "# fitted_models = {}\n",
    "# for name, model in models.items():\n",
    "#     print(f\"\\nTraining {name}...\")\n",
    "#     model.fit(X_train, y_train)\n",
    "#     fitted_models[name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training Gradient Boosted Trees...\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training MLP Classifier...\n",
      "\n",
      "Training Linear SVM...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        max_depth=10,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Gradient Boosted Trees\": GradientBoostingClassifier(n_estimators=20),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=300),\n",
    "    \"Linear SVM\": SVC(kernel=\"linear\", probability=True),\n",
    "    \"XGBoost\": xgb.XGBClassifier(n_estimators=50, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "fitted_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted_models[name] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation for {name}\")\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\")\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    evaluate(name, model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a66317",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\n",
    "    'amt': 100.0,\n",
    "    'lat': 37.7749,\n",
    "    'long': -122.4194,\n",
    "    'city_pop': 50000,\n",
    "    'unix_time': 1325376018,\n",
    "    'merch_lat': 37.0,\n",
    "    'merch_long': -122.0,\n",
    "    'trans_date_ts': 1577836800,\n",
    "    'category': 'misc_pos'\n",
    "}\n",
    "\n",
    "# Prepare user input\n",
    "user_input['category_index'] = le.transform([user_input['category']])[0]\n",
    "user_X = pd.DataFrame([user_input])[features]\n",
    "user_X = scaler.transform(user_X)\n",
    "\n",
    "# Predict with all models\n",
    "preds = []\n",
    "probs = []\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    prob = model.predict_proba(user_X)[0][1] if hasattr(model, \"predict_proba\") else None\n",
    "    pred = model.predict(user_X)[0]\n",
    "    preds.append(pred)\n",
    "    if prob is not None:\n",
    "        probs.append(prob)\n",
    "\n",
    "# Majority vote\n",
    "final_vote = round(sum(preds) / len(preds))\n",
    "final_prob = np.mean(probs)\n",
    "\n",
    "print(f\"\\nðŸ§  Ensemble Prediction for User Input:\")\n",
    "print(f\"Average Probability of Fraud: {final_prob:.4f}\")\n",
    "print(f\"Ensembled (Majority) Prediction: {'Fraud' if final_vote == 1 else 'Legit'}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
